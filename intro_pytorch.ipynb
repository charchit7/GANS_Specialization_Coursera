{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = torch.tensor([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 2\n",
      "no. of element 4\n"
     ]
    }
   ],
   "source": [
    "#to get rank, and no. of element\n",
    "print('rank', len(example_tensor.shape))\n",
    "print('no. of element', example_tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2]), tensor([1, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting column and rows.\n",
    "first_row  = example_tensor[0,:]\n",
    "first_column = example_tensor[:,0]\n",
    "first_row,first_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor//2 # applies broadcasting :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(example_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5198, 0.6477],\n",
       "        [0.7127, 0.1257]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0837, 0.4192])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1163, 0.3867])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tensor([[-0.2064, -1.1983,  1.0940, -1.9076, -0.9360,  1.3513,  0.0791,  2.1765,\n",
      "          0.0427,  0.0271],\n",
      "        [ 0.4900, -0.9306, -0.0106, -0.4529, -1.5940, -0.2319,  0.5281,  0.9652,\n",
      "          0.0851,  1.9323],\n",
      "        [-0.8620,  0.8580, -0.0613, -0.7515,  0.1324, -0.1783, -0.0952,  0.4113,\n",
      "          2.0565,  0.8771]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3426, -0.1449],\n",
       "        [ 0.4541, -0.4305],\n",
       "        [-0.8054, -0.0408]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working with nn module\n",
    "# nn.Linear performs Ax+b, note: a,x dimensions should match as per matrix multiplicaton property\n",
    "linear = nn.Linear(10,2) # take nx10 input size and outputs nx2 shape data.\n",
    "example = torch.randn(3,10)\n",
    "print('before',example)\n",
    "example = linear(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.4541, 0.0000],\n",
       "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Relu sets in general all negative terms goes 0.\n",
    "relu = nn.ReLU()\n",
    "rel_output = relu(example)\n",
    "rel_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2139,  0.3673],\n",
       "        [ 1.3176, -1.3661],\n",
       "        [-1.1037,  0.9988]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.BatchNorm1d rescales the batch of n inputs to have consistent mean and standard deviation\n",
    "btch  = nn.BatchNorm1d(2) # takes size of input dimenstions.\n",
    "btch_output = btch(example)\n",
    "btch_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0036, -0.0046],\n",
      "        [ 2.6728,  1.5010]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential used to perform all operation at once!\n",
    "test_data = torch.randn(2,2) + 1\n",
    "\n",
    "logistic_reg = nn.Sequential(\n",
    "    nn.Linear(2,1),\n",
    "    nn.BatchNorm1d(1), # note: dimension should be output of linear that is nx1 here\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "print(test_data)\n",
    "logistic_reg(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3300, -0.5779]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5638], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(logistic_reg.parameters()) # we can access the parameters like this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic calculation of gradient using torch.optim module.\n",
    "import torch.optim as optim\n",
    "adam = optim.Adam(logistic_reg.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8252, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7450, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7177, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "train_example = torch.randn(5,5) + 1\n",
    "\n",
    "\n",
    "architecture = nn.Sequential(\n",
    "    nn.Linear(5,2),\n",
    "    nn.BatchNorm1d(2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "#optimizer\n",
    "adam = optim.Adam(architecture.parameters(),lr=1e-2)\n",
    "\n",
    "for i in range(5):\n",
    "    adam.zero_grad()\n",
    "    curr_loss = torch.abs(1-architecture(train_example)).mean()\n",
    "    curr_loss.backward()\n",
    "    adam.step()\n",
    "    print(curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "842e6a94f7b00bd88c2d2a2fc81dcd9f0ee99928f96e0f860dbda703cd7a4701"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('mark1': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}